"""Wrapper optimizer for Elastic Average SGD """import tensorflow as tffrom tensorflow.python.training import optimizerfrom tensorflow.python.ops import resource_variable_opsLOCAL_VARIABLE_NAME = 'glocal_variable'GLOBAL_VARIABLE_NAME = 'global_variable'RECORD_LOCAL_VARIABLE_NAME = 'record_local_variable'class FlockingCustomGetter(object):    '''custom_getter is used with tf.get_variable instead of tf.Variablel.    This custom_getter is used to:    1. place the trainable variables(local_variables) to tower_device(gpu)    2. generate global_variables(a copy of_local variables) and place them on cpu,      which are untrainable are can be assessed by other tower to calculate the flocking_magnitude    3. generate record_local_variables which are untrainable and are placed on tower_device(gpu).     The number of record_local_variables is num_towers * local_variables. They are      used to record the global_variables of all towers(including the current tower).         For example,    flocking_custom_getter = FlockingCustomGetter(num_towers, tower_device)    with tf.variable_scope("tower_1", custom_getter=flocking_custom_getter):        weights = tf.get_variable(initializer=tf.truncated_normal([28*28, 30], stddev=1.0/28), name="weights")        biases = tf.get_variable(initializer=tf.zeros([30]), name="biases")    '''        def __init__(self, num_towers, tower_index):        '''create a new 'FlockingCustomGetter'.'''        self.num_towers = num_towers        self.tower_index = tower_index                ##a list store all the local_variables in the current tower.        #self.local_variables_list = []                ##a list store all the global_variables in the current tower.        #self.global_variables_list = []                 ##a list store all the record_local_variables including the current tower.        ##it is a record of all trainable variables        #self.record_local_variables_list= [[] for _ in range(num_towers)]              def __call__(self, getter, name, trainable, collections, *args, **kwargs):        if trainable:            with tf.device('/job:worker/task:%d/cpu:0' % self.tower_index):                local_variables = getter(                    name,                     trainable=True,                     collections=[tf.GraphKeys.LOCAL_VARIABLES, '%s_tower_%d' % (LOCAL_VARIABLE_NAME, self.tower_index)],                    *args,                    **kwargs)                            #self.local_variables_list.append(local_variables)                            # with tf.device('/job:ps/task:0/cpu:0'):            #     global_variables = tf.Variable(name='tower_%d_%s' % (self.tower_index, name),            #                                    initial_value=local_variables.initialized_value(),            #                                    trainable=False,            #                                    collections=[tf.GraphKeys.GLOBAL_VARIABLES,            #                                                 '%s_tower_%d' % (GLOBAL_VARIABLE_NAME, self.tower_index)])            # #self.global_variables_list.append(global_variables)            # tf.summary.histogram('global_variables_%s' % name, global_variables)            with tf.device('/job:worker/task:%d/cpu:0' % self.tower_index):                for i in range(self.num_towers):                    rlv =  tf.Variable(name='tower_%d_%s_%d' % (self.tower_index, name, i),                                      initial_value=local_variables.initialized_value(),                                      trainable=False,                                      collections=[tf.GraphKeys.LOCAL_VARIABLES,                                                   '%s_tower_%d' % (RECORD_LOCAL_VARIABLE_NAME, i)])                                #self.record_local_variables_list[i].append(rlv)                    #tf.summary.histogram('rlv', rlv)                    return local_variables        else:            return getter(name, trainable, collections, *args, **kwargs)class FlockingOptimizer(optimizer.Optimizer):    '''Wrapper optimizer that implements the Flocking SGD algorithm. This is an async    optimizer. During the training, each tower will copy all global_variables and stores    them at record_local_variables. And then each tower will calculate the average of     record_local_variables based on the 0 dimension which is actually the center for     each variable from different tower. Each tower then calcualte the distance between    local_variables and the average of record_local_variables. The distance is used to    calculate the flocking_magnitude to update both the local_variables and global_variables.     Also, after each tower updates the local_variables, the tower's own local_step will    be incremented by 1; after corresponding global_variables are updated, global_step will    be incremented by 1    '''        def __init__(self,                 opt,                 num_towers,                 tower_index,                 attraction = 0.0,                 repulsion = 0.0,                 dis = 0.001,                 use_locking=False,                 name='FlockingOptimizer'):        super(FlockingOptimizer, self).__init__(use_locking, name)        self._opt = opt        self._num_towers = num_towers        self._attra = attraction        self._repul = repulsion        self._dis = dis        #self._global_vars_all_towers = [fcg.global_variables_list for fcg in                                        #flocking_custom_getter_list]        #self._local_vars_list = [var for var in                                  #flocking_custom_getter_list[tower_index].local_variables_list]        #self._record_local_variables = flocking_custom_getter_list[tower_index].record_local_variables_list        self._slope = self._repul/self._dis        self._tower_index = tower_index                with tf.variable_scope('local_step_%d' % tower_index):            self._local_step = tf.get_variable(initializer=0,                                               trainable=False,                                               collections=[tf.GraphKeys.LOCAL_VARIABLES],                                               name='')        self._opt._prepare()            def compute_gradients(self,                          loss,                          gate_gradients=optimizer.Optimizer.GATE_OP,                          aggregation_method=None,                          colocate_gradients_with_ops=True,                          grad_loss=None):        '''compute gradients of 'loss' for variables in 'var_list', it returns         a list of (gradient, variable) pairs where "gradient" is the gradient for        "variable". Note that "gradient" can be a 'Tensor', an 'IndexedSlices'.or        'None' if there is no gradient for the given variable.                Args:        loss: A Tensor containing the value to minimize.        tower_index: the tower to update its local_variables.        gate_gradients: How to gate the computation of gradients.  Can be          `GATE_NONE`, `GATE_OP`, or `GATE_GRAPH`.        aggregation_method: Specifies the method used to combine gradient terms.          Valid values are defined in the class `AggregationMethod`.        colocate_gradients_with_ops: If True, try colocating gradients with          the corresponding op.        grad_loss: Optional. A `Tensor` holding the gradient computed for `loss`.      Returns:        A list of (gradient, variable) pairs. Variable is always present, but        gradient can be `None`.      Raises:        TypeError: If `var_list` contains anything else than `Variable` objects.        ValueError: If some arguments are invalid.        '''        return self._opt.compute_gradients(loss, tf.get_collection('%s_tower_%d' %(LOCAL_VARIABLE_NAME, self._tower_index)),                                             gate_gradients,                                            aggregation_method,                                            colocate_gradients_with_ops,                                            grad_loss)        def apply_gradients_and_flocking(self, grads_and_vars, global_step=None, name=None):        '''apply flocking_magnitude and gradients for local_variables of current tower'''        def _apply_flocking():            local_var_list = [v for g, v in grads_and_vars]            tower_device = local_var_list[0].device                        with tf.device(tower_device):                varr = tf.Print(local_var_list[0], [local_var_list[0]], 'this should be the first one, local variable is:')                global_var_update_after_gradient_update = [tf.assign(tf.get_collection_ref('%s_tower_%d' % (GLOBAL_VARIABLE_NAME, self._tower_index))[i],                                                                     local_var_list[i].read_value()) for i in range(len(local_var_list))]                with tf.control_dependencies(global_var_update_after_gradient_update):                    global_before_up = tf.Print(tf.get_default_graph().get_tensor_by_name("w1_tower_3:0"),                                        [tf.get_default_graph().get_tensor_by_name("w1_tower_3:0"),                                        tf.get_collection('%s_tower_%d' % (GLOBAL_VARIABLE_NAME, self._tower_index))[0].read_value(),                                         tf.get_default_graph().get_tensor_by_name("w1_tower_3:0").name,                                         tf.get_default_graph().get_tensor_by_name("w1_tower_3:0").shape],                                         'global variable after flob_ is:')                #global_var_update_ops0.append(glob_)                # for g_v, l_v in zip(tf.get_collection_ref('%s_tower_%d' % (GLOBAL_VARIABLE_NAME, self._tower_index)),                #                             local_var_list):                #     global_var_update_ops0.append(tf.assign(g_v, l_v.read_value()))            #global_var_update_ops0.append(varr)            # global_var_update_after_gradient_update.append(varr)            # global_var_update_after_gradient_update.append(global_before_up)            with tf.control_dependencies(global_var_update_after_gradient_update):                record_local_variables_update=[] # a list to store ops                with tf.device(tower_device):                    for i in range(self._num_towers):                        if i == self._tower_index:                            for rlv, lv in zip(tf.get_collection_ref('%s_tower_%d' % (RECORD_LOCAL_VARIABLE_NAME, i)),                                 local_var_list):                                record_local_variables_update.append(rlv.assign(lv.read_value()))                        else:                            for rlv, gv in zip(tf.get_collection_ref('%s_tower_%d' % (RECORD_LOCAL_VARIABLE_NAME, i)),                                               tf.get_collection('%s_tower_%d' % (GLOBAL_VARIABLE_NAME, i))):                                record_local_variables_update.append(rlv.assign(gv.read_value()))                                                with tf.control_dependencies(record_local_variables_update):                        same_variables_from_different_towers = zip(*(tf.get_collection_ref('%s_tower_%d' % (RECORD_LOCAL_VARIABLE_NAME, i))                                                                   for i in range(self._num_towers)))                        rv1 = tf.Print(same_variables_from_different_towers[0][0], [same_variables_from_different_towers[0][0].read_value()],                                         "this is in tower %d, record local for worker 0 is: " %(self._tower_index))                        rv2 = tf.Print(same_variables_from_different_towers[0][1], [same_variables_from_different_towers[0][1].read_value()],                                         "this is in tower %d, record local for worker 1 is: " %(self._tower_index))                        rv3 = tf.Print(same_variables_from_different_towers[0][2], [same_variables_from_different_towers[0][2].read_value()],                                         "this is in tower %d, record local for worker 2 is: " %(self._tower_index))                        rv4 = tf.Print(same_variables_from_different_towers[0][3], [same_variables_from_different_towers[0][3].read_value()],                                         "this is in tower %d, record local for worker 3 is: " %(self._tower_index))                        average_variables = [tf.add_n([i for i in s_v])/self._num_towers for s_v in same_variables_from_different_towers]                        av = tf.Print(average_variables[0], [average_variables[0]], "tower_%d, average variables is:" % self._tower_index)                                                    #vp = tf.Print(local_var_list[0], [local_var_list[0]], 'tower_%d, local variable is:'% self._tower_index)                        gp = tf.Print(grads_and_vars[0][0], [grads_and_vars[0][0]], 'tower_%d, gradients is: '% self._tower_index)                        distance = [tf.subtract(local.read_value(), average) for local, average in                                     zip(local_var_list, average_variables)]                                                di = tf.Print(distance[0], [distance[0]], 'tower_%d, Distance is: '% self._tower_index)                                 flocking_function = [tf.minimum(self._slope*tf.abs(dis)-self._repul+self._attra, self._attra)                                              for dis in distance]                        ff = tf.Print(flocking_function[0][0], [flocking_function[0][0]], 'tower_%d, flocking_function is: '% self._tower_index)                        flocking_magnitudes=[tf.multiply(dis, f) for dis, f in                                             zip(distance, flocking_function)]                        fm = tf.Print(flocking_magnitudes[0][0], [flocking_magnitudes[0][0]], 'tower_%d, flocking_magnitudes is: '% self._tower_index)                        local_var_update_ops = [tf.assign(var, var.read_value()-f_mag) for var, f_mag in                                                zip(local_var_list, flocking_magnitudes)]                        # local_var_update_ops.append(rv1)                        # local_var_update_ops.append(rv2)                        # local_var_update_ops.append(rv3)                        # local_var_update_ops.append(rv4)                        # local_var_update_ops.append(av)                        # #local_var_update_ops.append(vp)                                          # local_var_update_ops.append(di)                        # local_var_update_ops.append(gp)                        # local_var_update_ops.append(ff)                        # local_var_update_ops.append(fm)                                                with tf.control_dependencies(local_var_update_ops):                            v_up = tf.Print(local_var_list[0], [local_var_list[0]], "tower_%d, local_variables after update is: "% self._tower_index)                        with tf.control_dependencies(local_var_update_ops):                            with tf.device('/job:ps/task:0/cpu:0'):                                global_var_update_ops=[tf.assign(g_v, l_v.read_value()) for g_v, l_v in                                                       zip(tf.get_collection_ref('%s_tower_%d' % (GLOBAL_VARIABLE_NAME, self._tower_index)),                                                           local_var_list)]                            if global_step:                                with tf.colocate_with(global_step):                                    global_var_update_ops.append(tf.assign_add(global_step, 1))                            v_up = tf.Print(local_var_list[0], [local_var_list[0]], "tower_%d, local_variables after update is: "% self._tower_index)                                # global_var_update_ops.append(v_up)                            with tf.control_dependencies(global_var_update_ops):                                global_op = tf.Print(tf.get_collection('%s_tower_%d' % (GLOBAL_VARIABLE_NAME, self._tower_index))[0],                                    [tf.get_collection('%s_tower_%d' % (GLOBAL_VARIABLE_NAME, self._tower_index))[0].read_value()],                                     'this is tower %d, global variable after update is:'%self._tower_index)                                            var_update = tf.group(*(local_var_update_ops))            var_update1= tf.group(*(global_var_update_ops))            var_update2 = tf.group(var_update, var_update1)#, global_op)            return var_update2                ##apply gradients first            apply_gradients_ops = self._opt.apply_gradients(grads_and_vars)        # local_var_list = [v for g, v in grads_and_vars]        # tower_device = local_var_list[0].device        # with tf.device(tower_device):        #     gp = tf.Print(grads_and_vars[0][0], [grads_and_vars[0][0]], 'tower_%d, gradients is: '% self._tower_index)        # g = tf.group(apply_gradients_ops, gp)        #and then apply flocking_magnitude        with tf.control_dependencies([apply_gradients_ops]):            update = _apply_flocking()        return update